# -*- coding: utf-8 -*-
"""Lead_lag_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11SiZ5EvFrvNpfyygr7tmCm7jfwaAjeVM

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
–ò—Å—Ç–æ—á–Ω–∏–∫–∏:

–í —Å—Ç–∞—Ç—å–µ
  - https://www.crsp.org/research/crsp-us-stock-databases/ (–º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö)
  - https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html s include the market return (MKT), size (SMB), value (HML), profitability (RMW), investment (CMA), and momentumn (MOM)  (–ø–æ–∫–∞ –Ω–µ –ø–æ–Ω—è–ª –∑–∞—á–µ–º, –≤–æ–∑–º–æ–Ω–∂–Ω–æ, –¥–ª—è –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CAPM

–ù–∞—á–Ω–µ–º —Å–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å yahoofinance
"""

# !pip install yfinance -q
import yfinance as yf
import bs4 as bs
import numpy as np
import pandas as pd
import scipy.stats as stats
import os
import requests
from datetime import datetime
from pandas.core.frame import DataFrame
import matplotlib.pyplot as plt
import networkx as nx
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import plotly.express as px
from collections import defaultdict

"""$$r_{i,t}^{close-to-close} = P_{i,t}^{Adj close}/P_{i,t-1}^{Adj close} -1$$
$$r_{i,t}^{daytime} = P_{i,t}^{close}/P_{i,t}^{open} -1$$
$$r_{i,t}^{overnight} = (1+ r_{i,t}^{close-to-close})/(1+r_{i,t}^{daytime}) -1$$

$$œÅ_{i, j}^{overnight-lead-daytime} = Pearson.corr(r_{i,t}^{overnight}, r_{j,t}^{daytime})$$

$$œÅ_{i, j}^{daytime-lead-overnight} = Pearson.corr(r_{i,t-1}^{daytime}, r_{j,t}^{overnight})$$

"""

scrape_data = False # –ß—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—ã–π —Ä–∞–∑. False - –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –¥–∏—Å–∫–∞, True - —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–Ω–æ–≤–∞

def download_prices(tickers, start_date, end_date):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É
    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=False)

    returns_data = {}

    for ticker in tickers:
      df = pd.DataFrame({
          'close': data['Close'][ticker],
          'open': data['Open'][ticker],
          'adj_close': data['Adj Close'][ticker]
      }).dropna()

      # –†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
      df['close_to_close'] = df['adj_close'] / df['adj_close'].shift(1) - 1
      df['daytime'] = df['close'] / df['open'] - 1
      df['overnight'] = (1 + df['close_to_close']) / (1 + df['daytime']) - 1

      df['ticker'] = ticker
      returns_data[ticker] = df[['close_to_close', 'daytime', 'overnight', 'ticker']]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    all_returns = pd.concat(returns_data.values())
    return all_returns.reset_index().rename(columns={'index': 'date'})

if scrape_data:
  # –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤ —Å —Å–∞–π—Ç–∞
  resp = requests.get('https://stockanalysis.com/list/sp-500-stocks/')

  soup = bs.BeautifulSoup(resp.text, 'lxml')
  table = soup.find('table')
  tickers = []
  for row in table.findAll('tr')[1:]:
      ticker = row.findAll('td')[1].text
      industry = row.findAll('td')[1].text
      tickers.append(ticker)
  tickers = [s.replace('\n', '') for s in tickers]

  # sector
  df_sectors = pd.read_csv('/content/tickers.txt', sep='\t').rename(columns = {'Symbol': 'ticker'})
  df_sectors = df_sectors[['ticker', 'Industry', 'Sector']]
  returns_df = download_prices(tickers, '2020-01-01', '2024-12-31')
  returns_df = returns_df.merge(df_sectors, how = 'left', on = 'ticker')

  returns_df.to_parquet('/content/drive/MyDrive/–î–∏–ø–ª–æ–º_–º–∞–≥–∏—Å—Ç—Ä—Ç—É—Ä–∞/data.parquet')
else:
  returns_df = pd.read_parquet('/content/drive/MyDrive/–î–∏–ø–ª–æ–º_–º–∞–≥–∏—Å—Ç—Ä—Ç—É—Ä–∞/data.parquet', engine='pyarrow')

"""# –ú–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π

"""

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–∞—Ç—Ä–∏—Ü –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
def calculate_correlations(data, tickers, lookback_days=60):
    data = data.copy()
    data['Date'] = pd.to_datetime(data['Date'])

    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ lookback_days –¥–Ω–µ–π
    all_dates = sorted(data['Date'].unique())
    end_date = all_dates[-1]
    start_date = all_dates[-lookback_days]

    window_data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]

    n = len(tickers)
    corr_overnight_daytime = np.full((n, n), np.nan)
    corr_daytime_overnight = np.full((n, n), np.nan)

    overnight_pivot = window_data.pivot(index='Date', columns='ticker', values='overnight')
    daytime_pivot = window_data.pivot(index='Date', columns='ticker', values='daytime')

    for i in range(n):
        for j in range(n):
            ticker_i = tickers[i]
            ticker_j = tickers[j]

            # 1. Overnight_i -> Daytime_j
            df_ol_dt = pd.DataFrame({
                'overnight': overnight_pivot[ticker_i],
                'daytime': daytime_pivot[ticker_j]
            }).dropna()


            corr_ol_dt = stats.pearsonr(df_ol_dt['overnight'], df_ol_dt['daytime'])[0]
            corr_overnight_daytime[i,j] = corr_ol_dt

            # 2. Daytime_i -> Overnight_j (—Å–æ —Å–¥–≤–∏–≥–æ–º)
            df_dt_ol = pd.DataFrame({
                'daytime_shifted': daytime_pivot[ticker_i].shift(1),  # –°–¥–≤–∏–≥ –Ω–∞ 1 –¥–µ–Ω—å
                'overnight': overnight_pivot[ticker_j]
            }).dropna()


            corr_dt_ol = stats.pearsonr(df_dt_ol['daytime_shifted'], df_dt_ol['overnight'])[0]
            corr_daytime_overnight[i,j] = corr_dt_ol

    return corr_overnight_daytime, corr_daytime_overnight

# –¢–µ—Å—Ç
tickers_subset = ['NVDA', 'AAPL', 'MSFT', 'GOOGL', 'AMZN']
corr_overnight_daytime, corr_daytime_overnight = calculate_correlations(returns_df, tickers_subset)

print("Overnight-Lead-Daytime:")
print(pd.DataFrame(corr_overnight_daytime, index=tickers_subset, columns=tickers_subset).round(4))

print("\nDaytime-Lead-Overnight:")
print(pd.DataFrame(corr_daytime_overnight, index=tickers_subset, columns=tickers_subset).round(4))

"""## –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π

## T-test
"""

def test_correlation_significance(correlation, n_samples, alpha=0.05):
    """
    –¢–µ—Å—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ü–∏—Ä—Å–æ–Ω–∞

    H0: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è = 0 (–Ω–µ—Ç —Å–≤—è–∑–∏)
    H1: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è ‚â† 0 (–µ—Å—Ç—å —Å–≤—è–∑—å)
    """

    t_stat = correlation * np.sqrt(n_samples - 2) / np.sqrt(1 - correlation**2)
    p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=n_samples-2))

    return p_value

def sector_correlations_t_test(returns_df, lookback_days=60, alpha=0.05, min_correlation=0.2):
    results = []

    sector_counts = returns_df['Sector'].unique()

    for sector in sector_counts:
        # –¢–∏–∫–µ—Ä—ã –∏ –¥–∞–Ω–Ω—ã–µ —Å–µ–∫—Ç–æ—Ä–∞
        sector_tickers = returns_df[returns_df['Sector'] == sector]['ticker'].unique().tolist()
        sector_data = returns_df[returns_df['ticker'].isin(sector_tickers)]

        print(f"  {sector}: {len(sector_tickers)} –∞–∫—Ü–∏–π")

        # –í–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è
        corr_ol_dt, corr_dt_ol = calculate_correlations(sector_data, sector_tickers, lookback_days)

        # –¢–æ–ª—å–∫–æ —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        for i in range(len(sector_tickers)):
            for j in range(len(sector_tickers)):
                if i != j:
                    # OL‚ÜíDT
                    if not np.isnan(corr_ol_dt[i, j]) and abs(corr_ol_dt[i, j]) > min_correlation:
                        p_value = test_correlation_significance(corr_ol_dt[i, j], lookback_days)
                        if p_value < alpha:
                            results.append({
                                'sector': sector,
                                'i': sector_tickers[i],
                                'j': sector_tickers[j],
                                'corr': corr_ol_dt[i, j],
                                'p_value': p_value,
                                'type': 'overnight‚Üídaytime'
                            })

                if not np.isnan(corr_dt_ol[i, j]):
                    if abs(corr_dt_ol[i, j]) > min_correlation:
                        p_value = test_correlation_significance(corr_dt_ol[i, j], lookback_days)
                        if p_value < alpha:
                            results.append({
                                'sector': sector,
                                'i': sector_tickers[i],
                                'j': sector_tickers[j],
                                'corr': corr_dt_ol[i, j],
                                'p_value': p_value,
                                'type': 'daytime‚Üíovernight'
                            })

    return pd.DataFrame(results)

sector_results = sector_correlations_t_test(returns_df, lookback_days=60)

# sector_results.loc[sector_results['p_value'] < 0.05, 'significant'] = 'YES'
# sector_results.loc[sector_results['p_value'] > 0.05, 'significant'] = 'NO'
sector_results[sector_results['p_value'] > 0.05]
# sector_results.groupby(['type', 'sector', 'significant'])['correlation'].count()

"""## Bootstrap"""

def analyze_sectors_compact(returns_df, lookback_days=60, alpha=0.05,
                           n_bootstrap=200):


    results = []
    returns_df = returns_df.copy()

    sectors_to_analyze = returns_df['Sector'].unique()

    for sector in sectors_to_analyze:
        # –î–∞–Ω–Ω—ã–µ —Å–µ–∫—Ç–æ—Ä–∞
        sector_tickers = returns_df[returns_df['Sector'] == sector]['ticker'].unique().tolist()
        sector_data = returns_df[returns_df['ticker'].isin(sector_tickers)]

        print(f"\n{sector}: {len(sector_tickers)} –∞–∫—Ü–∏–π")

        # –ú–æ—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
        sector_data_dates = sector_data['Date'].unique()
        end_date = sorted(sector_data_dates)[-1]
        start_date = sorted(sector_data_dates)[-lookback_days]

        window = sector_data[
            (sector_data['Date'] >= start_date) &
            (sector_data['Date'] <= end_date)
        ]

        overnight = window.pivot(index='Date', columns='ticker', values='overnight')
        daytime = window.pivot(index='Date', columns='ticker', values='daytime')
        for i in range(len(sector_tickers)):
            for j in range(len(sector_tickers)):
              ticker_i = sector_tickers[i]
              ticker_j = sector_tickers[j]

              # OL‚ÜíDT
              df_ol_dt = pd.DataFrame({
                  'ol': overnight[ticker_i],
                  'dt': daytime[ticker_j]
              }).dropna()

              if len(df_ol_dt) >= 20:
                  # # Bootstrap —Ç–µ—Å—Ç
                  # x = df_ol_dt['ol'].values
                  # y = df_ol_dt['dt'].values
                  # n = len(x)

                  # –ù–∞–±–ª—é–¥–∞–µ–º–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                  corr_ol_dt = stats.pearsonr(df_ol_dt['ol'], df_ol_dt['dt'])[0]

                  # Bootstrap
                  bootstrap_corrs = []
                  for _ in range(n_bootstrap):
                      idx = np.random.permutation(len(df_ol_dt['ol']))
                      y_shuffled = df_ol_dt['dt'].values[idx]
                      corr = stats.spearmanr(df_ol_dt['ol'].values, y_shuffled)[0]
                      bootstrap_corrs.append(corr)

                  # p-value
                  p_value = np.mean(np.abs(bootstrap_corrs) >= np.abs(corr_ol_dt))


                  if p_value < alpha:
                      results.append({
                          'sector': sector,
                          'type': 'overnight‚Üídaytime',
                          'i': ticker_i,
                          'j': ticker_j,
                          'correlation_pearson': corr_ol_dt,
                          # 'correlation_spearman': observed_corr,
                          'p_value': p_value
                          # 'n_obs': n
                      })

              # DT‚ÜíOL (—Å–æ —Å–¥–≤–∏–≥–æ–º)
              df_dt_ol = pd.DataFrame({
                  'dt': daytime[ticker_i].shift(1),
                  'ol': overnight[ticker_j]
              }).dropna()

              if len(df_ol_dt) >= 20:

                  # –ù–∞–±–ª—é–¥–∞–µ–º–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                  corr_ol_dt = stats.pearsonr(df_ol_dt['ol'], df_ol_dt['dt'])[0]

                  # Bootstrap
                  bootstrap_corrs = []
                  for _ in range(n_bootstrap):
                      idx = np.random.permutation(len(df_ol_dt['ol']))
                      y_shuffled = df_ol_dt['dt'].values[idx]
                      corr = stats.spearmanr(df_ol_dt['ol'].values, y_shuffled)[0]
                      bootstrap_corrs.append(corr)

                  # p-value
                  p_value = np.mean(np.abs(bootstrap_corrs) >= np.abs(corr_ol_dt))


                  if p_value < alpha:
                      results.append({
                          'sector': sector,
                          'type': 'overnight‚Üídaytime',
                          'i': ticker_i,
                          'j': ticker_j,
                          'correlation_pearson': corr_ol_dt,
                          # 'correlation_spearman': observed_corr,
                          'p_value': p_value
                          # 'n_obs': n
                      })
    df_results = pd.DataFrame(results)


    return df_results

results = analyze_sectors_compact(
    returns_df,
    lookback_days=60,              # 60 –¥–Ω–µ–π
    alpha=0.05,                    # 5% —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    n_bootstrap=200               # 200 –∏—Ç–µ—Ä–∞—Ü–∏–π bootstrap
)

results.to_csv('bootstrap_result.csv')







"""# –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è"""

class LeadLagAnalyzer:
    def __init__(self, corr_ol_dt, corr_dt_ol, tickers):
        self.corr_ol_dt = corr_ol_dt
        self.corr_dt_ol = corr_dt_ol
        self.tickers = tickers

    def analyze_all(self, edge_threshold=0.1):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π Streamlit –¥–∞—à–±–æ—Ä–¥—É"""
        print("LEAD-LAG ANALYSIS ===\n")

        # 1. Heatmaps
        self.plot_heatmaps()

        # 2. Network Analysis
        self.analyze_networks(edge_threshold)

        # 3. Lead-Lag Patterns
        self.analyze_patterns()

        # 4. Detailed Statistics
        self.print_detailed_stats()

    def plot_heatmaps(self):
        """–¢–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        print("HEATMAPS")

        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("Overnight-Lead-Daytime", "Daytime-Lead-Overnight")
        )

        # Heatmap 1: Overnight-Lead-Daytime
        fig.add_trace(
            go.Heatmap(
                z=self.corr_ol_dt,
                x=self.tickers,
                y=self.tickers,
                colorscale='RdBu_r',
                zmid=0
            ),
            row=1, col=1
        )

        # Heatmap 2: Daytime-Lead-Overnight
        fig.add_trace(
            go.Heatmap(
                z=self.corr_dt_ol,
                x=self.tickers,
                y=self.tickers,
                colorscale='RdBu_r',
                zmid=0
            ),
            row=1, col=2
        )

        fig.update_layout(height=500, title_text="Correlation Matrices")
        fig.show()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–∞—Ç—Ä–∏—Ü–∞–º
        self._print_matrix_stats()

    def _print_matrix_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü"""
        ol_nonzero = self.corr_ol_dt[self.corr_ol_dt != 0]
        dt_nonzero = self.corr_dt_ol[self.corr_dt_ol != 0]

        print(f"\n MATRIX STATISTICS:")
        print(f"Overnight-Lead-Daytime:")
        print(f"  Mean: {np.mean(ol_nonzero):.4f}")
        print(f"  Max:  {np.max(self.corr_ol_dt):.4f}")
        print(f"  Min:  {np.min(self.corr_ol_dt):.4f}")
        print(f"  Std:  {np.std(ol_nonzero):.4f}")

        print(f"Daytime-Lead-Overnight:")
        print(f"  Mean: {np.mean(dt_nonzero):.4f}")
        print(f"  Max:  {np.max(self.corr_dt_ol):.4f}")
        print(f"  Min:  {np.min(self.corr_dt_ol):.4f}")
        print(f"  Std:  {np.std(dt_nonzero):.4f}")

    def analyze_networks(self, threshold=0.1):
        """–ê–Ω–∞–ª–∏–∑ —Å–µ—Ç–µ–π"""
        print(f"\n NETWORK ANALYSIS (threshold: {threshold})")

        # –ê–Ω–∞–ª–∏–∑ –æ–±–µ–∏—Ö —Å–µ—Ç–µ–π
        for matrix_type, matrix in [("Overnight-Lead-Daytime", self.corr_ol_dt),
                                   ("Daytime-Lead-Overnight", self.corr_dt_ol)]:
            print(f"\n--- {matrix_type} ---")
            G = self._build_network(matrix, threshold)

            if G.number_of_edges() > 0:
                self._plot_network(G, matrix_type)
                self._print_network_stats(G, matrix_type)
            else:
                print("  No significant edges found")

    def _build_network(self, correlation_matrix, threshold):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞"""
        G = nx.DiGraph()

        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
        for ticker in self.tickers:
            G.add_node(ticker)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–±—Ä–∞
        for i in range(len(self.tickers)):
            for j in range(len(self.tickers)):
                if i != j and abs(correlation_matrix[i, j]) > threshold:
                    G.add_edge(self.tickers[i], self.tickers[j],
                              weight=correlation_matrix[i, j])

        return G

    def _plot_network(self, G, title):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ç–∏"""
        plt.figure(figsize=(12, 8))

        # –ü–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∑–ª–æ–≤
        pos = nx.spring_layout(G, k=2, iterations=50)

        # –†–∞–∑–º–µ—Ä —É–∑–ª–æ–≤ –ø–æ –∏—Å—Ö–æ–¥—è—â–µ–π —Å—Ç–µ–ø–µ–Ω–∏
        out_degree = dict(G.out_degree())
        node_sizes = [300 + out_degree[node] * 100 for node in G.nodes()]

        # –¶–≤–µ—Ç —É–∑–ª–æ–≤ –ø–æ –∏—Å—Ö–æ–¥—è—â–µ–π —Å—Ç–µ–ø–µ–Ω–∏
        node_colors = list(out_degree.values())

        # –†–∏—Å—É–µ–º —Ä–µ–±—Ä–∞
        edge_colors = ['red' if G[u][v]['weight'] < 0 else 'green'
                      for u, v in G.edges()]
        edge_widths = [abs(G[u][v]['weight']) * 3 for u, v in G.edges()]

        nx.draw_networkx_edges(G, pos, edge_color=edge_colors,
                              width=edge_widths, alpha=0.6,
                              arrows=True, arrowsize=20)

        # –†–∏—Å—É–µ–º —É–∑–ª—ã
        nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes,
                                     node_color=node_colors,
                                     cmap='viridis', alpha=0.8)

        # –ü–æ–¥–ø–∏—Å–∏
        nx.draw_networkx_labels(G, pos, font_size=8)

        plt.colorbar(nodes, label='Out-degree (Leadership)')
        plt.title(f'{title} Network')
        plt.axis('off')
        plt.tight_layout()
        plt.show()

    def _print_network_stats(self, G, network_type):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Ç–∏"""
        out_degrees = dict(G.out_degree())
        in_degrees = dict(G.in_degree())

        print(f"  Nodes: {G.number_of_nodes()}")
        print(f"  Edges: {G.number_of_edges()}")
        print(f"  Density: {nx.density(G):.4f}")

        # –¢–æ–ø –ª–∏–¥–µ—Ä—ã
        top_leaders = sorted(out_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
        print(f"  Top Leaders: {[f'{ticker}({deg})' for ticker, deg in top_leaders]}")

        # –¢–æ–ø –ª–∞–≥–µ—Ä—ã
        top_laggers = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
        print(f"  Top Laggers: {[f'{ticker}({deg})' for ticker, deg in top_laggers]}")

    def analyze_patterns(self):
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ lead-lag"""
        print(f"\nüìà LEAD-LAG PATTERNS")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        ol_corrs = self.corr_ol_dt[self.corr_ol_dt != 0].flatten()
        dt_corrs = self.corr_dt_ol[self.corr_dt_ol != 0].flatten()

        fig = make_subplots(rows=1, cols=2, subplot_titles=[
            "Overnight-Lead-Daytime", "Daytime-Lead-Overnight"
        ])

        fig.add_trace(go.Histogram(x=ol_corrs, name="OL‚ÜíDT", nbinsx=30), row=1, col=1)
        fig.add_trace(go.Histogram(x=dt_corrs, name="DT‚ÜíOL", nbinsx=30), row=1, col=2)

        fig.update_layout(height=400, title_text="Correlation Distributions")
        fig.show()

        # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è
        self._print_asymmetry_analysis(ol_corrs, dt_corrs)

    def _print_asymmetry_analysis(self, ol_corrs, dt_corrs):
        """–ê–Ω–∞–ª–∏–∑ –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏"""
        ol_mean = np.mean(np.abs(ol_corrs))
        dt_mean = np.mean(np.abs(dt_corrs))

        ol_positive = np.sum(ol_corrs > 0)
        ol_negative = np.sum(ol_corrs < 0)
        dt_positive = np.sum(dt_corrs > 0)
        dt_negative = np.sum(dt_corrs < 0)

        print(f"\n ASYMMETRY ANALYSIS:")
        print(f"Overnight-Lead-Daytime:")
        print(f"  Avg |correlation|: {ol_mean:.4f}")
        print(f"  Positive/Negative: {ol_positive}/{ol_negative}")

        print(f"Daytime-Lead-Overnight:")
        print(f"  Avg |correlation|: {dt_mean:.4f}")
        print(f"  Positive/Negative: {dt_positive}/{dt_negative}")

        if ol_mean > dt_mean:
            print(" TUG-OF-WAR EVIDENCE: Overnight-lead relationships are stronger!")
        else:
            print("‚Ñπ Daytime-lead relationships are stronger")

    def print_detailed_stats(self):
        """–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        print(f"\n DETAILED STATISTICS")

        # –°–∞–º—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
        print(" STRONGEST LEAD-LAG RELATIONSHIPS:")

        for matrix_type, matrix in [("OL‚ÜíDT", self.corr_ol_dt),
                                   ("DT‚ÜíOL", self.corr_dt_ol)]:
            print(f"\n{matrix_type}:")
            strong_pairs = []

            for i in range(len(self.tickers)):
                for j in range(len(self.tickers)):
                    if i != j and not np.isnan(matrix[i, j]):
                        strong_pairs.append((self.tickers[i], self.tickers[j], matrix[i, j]))

            # –¢–æ–ø 5 —Å–∞–º—ã—Ö —Å–∏–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π
            top_pairs = sorted(strong_pairs, key=lambda x: abs(x[2]), reverse=True)[:5]
            for leader, lagger, corr in top_pairs:
                direction = "‚Üë‚Üë" if corr > 0 else "‚Üë‚Üì"
                print(f"  {leader} ‚Üí {lagger}: {corr:.4f} {direction}")

    def analyze_stock_pair(self, stock1, stock2):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã –∞–∫—Ü–∏–π"""
        if stock1 not in self.tickers or stock2 not in self.tickers:
            print("Stocks not found in ticker list")
            return

        idx1 = self.tickers.index(stock1)
        idx2 = self.tickers.index(stock2)

        ol_dt_corr = self.corr_ol_dt[idx1, idx2]
        dt_ol_corr = self.corr_dt_ol[idx1, idx2]

        print(f"\n PAIR ANALYSIS: {stock1} vs {stock2}")
        print(f"Overnight {stock1} ‚Üí Daytime {stock2}: {ol_dt_corr:.4f}")
        print(f"Daytime {stock1} ‚Üí Overnight {stock2}: {dt_ol_corr:.4f}")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        if abs(ol_dt_corr) > 0.2:
            direction = "positive" if ol_dt_corr > 0 else "negative"
            print(f" Strong {direction} lead-lag relationship detected!")

# –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï
print("=== LEAD-LAG ANALYSIS STARTING ===")

corr_overnight_daytime, corr_daytime_overnight = calculate_correlations(returns_df, tickers)

# –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
analyzer = LeadLagAnalyzer(corr_overnight_daytime, corr_daytime_overnight, tickers)

analyzer.analyze_all(edge_threshold=0.1)

# # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–∞—Ä
# analyzer.analyze_stock_pair('NVDA', 'AAPL')
# analyzer.analyze_stock_pair('MSFT', 'GOOGL')

"""Heatmap –Ω–µ –∑–Ω–∞—é –∫–∞–∫–∏–µ –≤—ã–≤–æ–¥—ã

–ü–æ  _print_matrix_stats –ê—Å–∏–º–º–µ—Ç—Ä–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π:
 - Overnight‚ÜíDaytime: -0.0096 (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ)
 - Daytime‚ÜíOvernight: +0.0034 (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ)

NETWORK ANALYSIS
  - –ø–ª–æ—Ç–Ω–æ—Å—Ç—å 50%
  - –º–æ–∂–µ–º –≤—ã–±—Ä–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥, 0.1 –º–∞–ª–æ
  - Overnight-Lead-Daytime: –µ—Å–ª–∏ JBL —Å–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç –Ω–æ—á—å—é, —Ç–æ –¥–Ω–µ–º —Ä–æ—Å—Ç TRMB

–Ω–æ—á–Ω—ã–µ –≤–æ–∑–≤—Ä–∞—Ç—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —è–≤–ª—è—é—Ç—Å—è –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–º–∏ –ª–∏–¥–µ—Ä–∞–º–∏:
OL‚ÜíDT: 0.1212 vs DT‚ÜíOL: 0.1071, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤—ã–≤–æ–¥–∞–º —Å—Ç–∞—Ç—å–∏ –æ "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ" –Ω–æ—á–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤

–°–µ—Ç–µ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —á–µ—Ç–∫–∏–µ lead-lag –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∞–∫—Ü–∏—è–º–∏:
–°–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ —Ç–∏–ø–∞ WFC ‚Üí TRMB (0.8126)

"""
